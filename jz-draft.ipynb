{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a variable clustering algorithm that is similar to SAS varclus\n",
    "1. PCA-based recursive decomposition\n",
    "2. stopping critiera\n",
    "3. plotting\n",
    "4. OOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake data\n",
    "feature_df, label_df = make_classification(n_samples=int(1e5), n_features=500)\n",
    "feature_df = scale(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6914913913979035e-16, 1.0000050000375227)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure data is zscored\n",
    "np.max(np.abs(feature_df.mean(axis=0))), np.max((feature_df.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(feature_df, columns=['feature_' + str(i) for i in range(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_step_clustering(df):\n",
    "    # First two components\n",
    "    num_split = 2\n",
    "    \n",
    "    # TODO: make a class to wrap pca features\n",
    "    pca = PCA(n_components=num_split)\n",
    "    _ = pca.fit(df)\n",
    "    pca_features = []\n",
    "    pca_corr = []\n",
    "    clusters = {}\n",
    "    \n",
    "    for i in range(num_split):\n",
    "        pca_features.append(df.dot(pca.components_[i]))\n",
    "        pca_corr.append(df.corrwith(pca_features[i]) ** 2)\n",
    "        \n",
    "    # Initial assignment\n",
    "    corr_table = pd.concat(pca_corr, axis=1)\n",
    "    corr_max = corr_table.max(axis=1)\n",
    "    cluster_membership = corr_table.apply(lambda x: x == corr_max)\n",
    "    \n",
    "    for i in range(num_split):\n",
    "        clusters['cluster_{}'.format(i)] = \\\n",
    "            [feature \n",
    "             for (feature, condition) in cluster_membership[i].to_dict().items() \n",
    "             if condition]\n",
    "            \n",
    "    return clusters, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_1_0, pca = one_step_clustering(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0_df = feature_df[step_1_0['cluster_0']]\n",
    "cluster_1_df = feature_df[step_1_0['cluster_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_reassign(clus_0, clus_1, feature_to_clus1):\n",
    "    \"\"\"\n",
    "    Tries to re-assign a feature from cluster 0 to cluster 1\n",
    "    \"\"\"\n",
    "    \n",
    "    pca = PCA(n_components=1)\n",
    "    \n",
    "    # TODO: parallelization \n",
    "    total_variance_explained = \\\n",
    "        pca.fit(clus_0).explained_variance_[0] \\\n",
    "        + pca.fit(clus_1).explained_variance_[0]\n",
    "        \n",
    "    new_clus_0 = clus_0.drop(feature_to_clus1, axis=1)\n",
    "    new_clus_1 = clus_1.join(clus_0[feature_to_clus1])\n",
    "    \n",
    "    new_total_variance_explained = \\\n",
    "        pca.fit(new_clus_0).explained_variance_[0] \\\n",
    "        + pca.fit(new_clus_1).explained_variance_[0]\n",
    "        \n",
    "    return (new_total_variance_explained > total_variance_explained), \\\n",
    "           new_total_variance_explained, \\\n",
    "           total_variance_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "try_reassign(cluster_0_df, cluster_1_df, cluster_0_df.columns[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
