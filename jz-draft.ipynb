{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a variable clustering algorithm that is similar to SAS varclus\n",
    "1. PCA-based recursive decomposition\n",
    "2. stopping critiera\n",
    "3. plotting\n",
    "4. OOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake data\n",
    "feature_df, label_df = make_classification(n_samples=int(1e5), n_features=500)\n",
    "feature_df = scale(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure data is zscored\n",
    "np.max(np.abs(feature_df.mean(axis=0))), np.max((feature_df.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(feature_df, columns=['feature_' + str(i) for i in range(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_step_clustering(df):\n",
    "    # First two components\n",
    "    num_split = 2\n",
    "    \n",
    "    # TODO: make a class to wrap pca features\n",
    "    pca = PCA(n_components=num_split)\n",
    "    _ = pca.fit(df)\n",
    "    pca_features = []\n",
    "    pca_corr = []\n",
    "    clusters = {}\n",
    "    \n",
    "    for i in range(num_split):\n",
    "        pca_features.append(df.dot(pca.components_[i]))\n",
    "        pca_corr.append(df.corrwith(pca_features[i]) ** 2)\n",
    "        \n",
    "    # Initial assignment\n",
    "    corr_table = pd.concat(pca_corr, axis=1)\n",
    "    corr_max = corr_table.max(axis=1)\n",
    "    cluster_membership = corr_table.apply(lambda x: x == corr_max)\n",
    "    \n",
    "    for i in range(num_split):\n",
    "        clusters['cluster_{}'.format(i)] = \\\n",
    "            [feature \n",
    "             for (feature, condition) in cluster_membership[i].to_dict().items() \n",
    "             if condition]\n",
    "            \n",
    "    return clusters, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_1_0, pca = one_step_clustering(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0_df = feature_df[step_1_0['cluster_0']]\n",
    "cluster_1_df = feature_df[step_1_0['cluster_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_reassign(clus_0, clus_1, feature_to_clus1):\n",
    "    \"\"\"\n",
    "    Tries to re-assign a feature from cluster 0 to cluster 1\n",
    "    \"\"\"\n",
    "    \n",
    "    pca = PCA(n_components=1)\n",
    "    \n",
    "    # TODO: parallelization \n",
    "    total_variance_explained = \\\n",
    "        pca.fit(clus_0).explained_variance_[0] \\\n",
    "        + pca.fit(clus_1).explained_variance_[0]\n",
    "        \n",
    "    new_clus_0 = clus_0.drop(feature_to_clus1, axis=1)\n",
    "    new_clus_1 = clus_1.join(clus_0[feature_to_clus1])\n",
    "    \n",
    "    new_total_variance_explained = \\\n",
    "        pca.fit(new_clus_0).explained_variance_[0] \\\n",
    "        + pca.fit(new_clus_1).explained_variance_[0]\n",
    "        \n",
    "    return (new_total_variance_explained > total_variance_explained), \\\n",
    "           new_total_variance_explained, \\\n",
    "           total_variance_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 3.4376880491231034, 3.4324589788986684)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "try_reassign(cluster_0_df, cluster_1_df, cluster_0_df.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "Basic layout is done, however, there is some re-factorization needed\n",
    "* OOP: encapsulate the clustering in a class similar to sklearn.decomposition.PCA\n",
    "* Optimization. Most of the PCA calculations above are certainly redundant. Within each function block, some parallelization could be done to improve efficiency\n",
    "* Add a unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# POC\n",
    "class VarClus:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def varclus(self):\n",
    "        def one_step:\n",
    "            pass\n",
    "        \n",
    "        def re_assign:\n",
    "            pass\n",
    "        \n",
    "    def plot(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
