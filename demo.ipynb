{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "> **The purpose of this notebook is to demostrate**\n",
    "> 1. How to conduct a feature space decomposition\n",
    "> 2. How to visualize a decomposition\n",
    "> 3. Qualitatively validate if the algorithm performs as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from decomposition.var_clus import VarClus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 1: Instantiate the VarClus class\n",
    "\n",
    "> **The parameters used in the constructor are**  \n",
    "\n",
    "> **`n_split`**: Number of sub-clusters that every time a cluster is split into. Default 2  \n",
    "**`max_eigenvalue`**: Eigenvalue threshold below which the decomposition will be stopped. Please note, the dataframe will be scaled during the process so each of the features will have variance == 1. Default 1   \n",
    "**`max_tries`**: Number of max tries before the algorithm gives up. Default 3\n",
    "\n",
    "> Besides the aformentioned properties, a key property is called Cluster. A Cluster object holds the information related to the below regards\n",
    "1. What features are in the cluster\n",
    "2. What are the parent clusters, if any\n",
    "3. what are the child clusters, in any\n",
    "4. Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo1 = VarClus()\n",
    "\n",
    "# Larger max_eigenvalue usually results in bigger and fewer child clusters\n",
    "demo1 = VarClus(max_eigenvalue=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2: Test on an arbitrary dataset\n",
    "\n",
    "> Let's create some simple dataset to play with  \n",
    "> We can leverage make_classification to make the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decomposing cluster cluster-0\n",
      "phase #1: NCS\n",
      "phase #2: Search\n",
      "assessing feature feature_10\n",
      "current EV is 3.7924900743028194, new EV is 3.793246884573982\n",
      "Feature feature_10 was re-assigned\n",
      "child_clusters[i] has 11 features and child_clusters[j] has 14 features\n",
      "assessing feature feature_14\n",
      "current EV is 3.793246884573982, new EV is 3.619450861908802\n",
      "assessing feature feature_15\n",
      "current EV is 3.793246884573982, new EV is 3.7180395740211196\n",
      "assessing feature feature_2\n",
      "current EV is 3.793246884573982, new EV is 3.7555131463259404\n",
      "Number of max tries has been reached. Returning current result...\n",
      "decomposing cluster cluster-0-0\n",
      "phase #1: NCS\n",
      "phase #2: Search\n",
      "assessing feature feature_14\n",
      "current EV is 2.991324113673583, new EV is 2.8614019129000443\n",
      "assessing feature feature_15\n",
      "current EV is 2.991324113673583, new EV is 2.973809988914784\n",
      "assessing feature feature_2\n",
      "current EV is 2.991324113673583, new EV is 2.96300695940678\n",
      "Number of max tries has been reached. Returning current result...\n",
      "decomposing cluster cluster-0-0-0\n",
      "phase #1: NCS\n",
      "phase #2: Search\n",
      "assessing feature feature_14\n",
      "current EV is 2.4979610464095083, new EV is 2.601176502049789\n",
      "Feature feature_14 was re-assigned\n",
      "child_clusters[i] has 3 features and child_clusters[j] has 4 features\n",
      "assessing feature feature_2\n",
      "current EV is 2.601176502049789, new EV is 2.372250867820816\n",
      "assessing feature feature_23\n",
      "current EV is 2.601176502049789, new EV is 2.7097419794277497\n",
      "Feature feature_23 was re-assigned\n",
      "child_clusters[i] has 2 features and child_clusters[j] has 5 features\n",
      "assessing feature feature_9\n",
      "Number of features is smaller than n_split, cannot conduct PCA\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'explained_variance_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-dcf92c7e8d74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mdemo2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVarClus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_eigenvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdemo2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdemo2_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\projects\\variable-clustering\\decomposition\\var_clus.py\u001b[0m in \u001b[0;36mdecompose\u001b[1;34m(self, dataframe)\u001b[0m\n\u001b[0;32m    422\u001b[0m         VarClus._decompose(self.cluster,\n\u001b[0;32m    423\u001b[0m                            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_eigenvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m                            self.max_tries)\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\projects\\variable-clustering\\decomposition\\var_clus.py\u001b[0m in \u001b[0;36m_decompose\u001b[1;34m(cluster, max_eigenvalue, max_tries)\u001b[0m\n\u001b[0;32m    405\u001b[0m                     VarClus._decompose(child_cluster,\n\u001b[0;32m    406\u001b[0m                                        \u001b[0mmax_eigenvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m                                        max_tries)\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\projects\\variable-clustering\\decomposition\\var_clus.py\u001b[0m in \u001b[0;36m_decompose\u001b[1;34m(cluster, max_eigenvalue, max_tries)\u001b[0m\n\u001b[0;32m    405\u001b[0m                     VarClus._decompose(child_cluster,\n\u001b[0;32m    406\u001b[0m                                        \u001b[0mmax_eigenvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m                                        max_tries)\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\projects\\variable-clustering\\decomposition\\var_clus.py\u001b[0m in \u001b[0;36m_decompose\u001b[1;34m(cluster, max_eigenvalue, max_tries)\u001b[0m\n\u001b[0;32m    400\u001b[0m            \u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mmax_eigenvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'decomposing cluster {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                 \u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVarClus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_step_decompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_tries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mchild_cluster\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\projects\\variable-clustering\\decomposition\\var_clus.py\u001b[0m in \u001b[0;36mone_step_decompose\u001b[1;34m(cluster, max_tries)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'phase #2: Search'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[0mchild_clusters\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             \u001b[0mVarClus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreassign_features_pca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_tries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mchild_clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\projects\\variable-clustering\\decomposition\\var_clus.py\u001b[0m in \u001b[0;36mreassign_features_pca\u001b[1;34m(child_clusters, max_tries)\u001b[0m\n\u001b[0;32m    235\u001b[0m                                                          \u001b[0mchild_clusters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                                                          \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                                                          other_clusters)\n\u001b[0m\u001b[0;32m    238\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mchange_flag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Feature {} was re-assigned'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\projects\\variable-clustering\\decomposition\\var_clus.py\u001b[0m in \u001b[0;36mreassign_one_feature_pca\u001b[1;34m(cluster_from, cluster_to, feature, other_clusters)\u001b[0m\n\u001b[0;32m    188\u001b[0m         explained_variance_after_assignment = np.sum(\n\u001b[0;32m    189\u001b[0m             [cluster.pca.explained_variance_[0] for cluster in\n\u001b[1;32m--> 190\u001b[1;33m              [cluster_from_new, cluster_to_new] + other_clusters],\n\u001b[0m\u001b[0;32m    191\u001b[0m         )\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\projects\\variable-clustering\\decomposition\\var_clus.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         explained_variance_after_assignment = np.sum(\n\u001b[1;32m--> 189\u001b[1;33m             [cluster.pca.explained_variance_[0] for cluster in\n\u001b[0m\u001b[0;32m    190\u001b[0m              [cluster_from_new, cluster_to_new] + other_clusters],\n\u001b[0;32m    191\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'explained_variance_'"
     ]
    }
   ],
   "source": [
    "n_features = 25\n",
    "n_rows = 1e4\n",
    "\n",
    "raw_df, _ = make_classification(n_samples=int(n_rows), \n",
    "                                n_features=n_features, \n",
    "                                n_informative=n_features,\n",
    "                                n_redundant=0)\n",
    "\n",
    "columns = ['feature_{}'.format(i) for i in range(n_features)]\n",
    "demo2_df = pd.DataFrame(raw_df, columns=columns)\n",
    "\n",
    "demo2 = VarClus(max_eigenvalue=1.1)\n",
    "demo2.decompose(demo2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The decomposition has a hierarchical structure, meaning the features of a child cluster are a subset of its parent cluster.\n",
    "The whole algorithm can be described as below**\n",
    "\n",
    ">> 1. Conducts PCA on current feature space. If the max eigenvalue is smaller than threshold,\n",
    "    stop decomposition\n",
    "2. Calculates the first N PCA components and assign features to these components based on\n",
    "    absolute correlation from high to low. These components are the initial centroids of\n",
    "    these child clusters.\n",
    "3. After initial assignment, the algorithm conducts an iterative assignment called Nearest\n",
    "    Component Sorting (NCS). Basically, the centroid vectors are re-computed as the first\n",
    "    components of the child clusters and the algorithm will re-assign each of the feature\n",
    "    based on the same correlation rule.\n",
    "4. After NCS, the algorithm tries to increase the total variance explained by the first\n",
    "    PCA component of each child cluster by re-assigning features across clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checkout the root_cluster\n",
    "root_cluster = demo2.cluster\n",
    "\n",
    "# Direct children of the root_cluster\n",
    "child_clusters = root_cluster.children\n",
    "\n",
    "# Direct parent of the root_cluster, if any\n",
    "parent_clusters = root_cluster.parents\n",
    "\n",
    "# root_cluster contains the original dataframe\n",
    "root_cluster.dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print out the structure of the decomposition\n",
    "demo2.print_cluster_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
