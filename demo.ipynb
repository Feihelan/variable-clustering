{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "> **The purpose of this notebook is to demostrate**\n",
    "> 1. How to conduct a feature space decomposition\n",
    "> 2. How to visualize a decomposition\n",
    "> 3. Qualitatively validate if the algorithm performs as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from decomposition.var_clus import VarClus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 1: Instantiate the VarClus class\n",
    "\n",
    "> **The parameters used in the constructor are**  \n",
    "\n",
    "> **`n_split`**: Number of sub-clusters that every time a cluster is split into. Default 2  \n",
    "**`max_eigenvalue`**: Eigenvalue threshold below which the decomposition will be stopped. Please note, the dataframe will be scaled during the process so each of the features will have variance == 1. Default 1   \n",
    "**`max_tries`**: Number of max tries before the algorithm gives up. Default 3\n",
    "\n",
    "> Besides the aformentioned properties, a key property is called Cluster. A Cluster object holds the information related to the below regards\n",
    "1. What features are in the cluster\n",
    "2. What are the parent clusters, if any\n",
    "3. what are the child clusters, in any\n",
    "4. Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo1 = VarClus()\n",
    "\n",
    "# Larger max_eigenvalue usually results in bigger and fewer child clusters\n",
    "demo1 = VarClus(max_eigenvalue=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2: Test on an arbitrary dataset\n",
    "\n",
    "> Let's create some simple dataset to play with  \n",
    "> We can leverage make_classification to make the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features is smaller than n_split, cannot conduct PCA\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'explained_variance_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-dcf92c7e8d74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mdemo2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVarClus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_eigenvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdemo2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdemo2_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\projects\\variable-clustering\\decomposition\\var_clus.py\u001b[0m in \u001b[0;36mdecompose\u001b[1;34m(self, dataframe)\u001b[0m\n\u001b[0;32m    423\u001b[0m         VarClus._decompose(self.cluster,\n\u001b[0;32m    424\u001b[0m                            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_eigenvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m                            self.max_tries)\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\projects\\variable-clustering\\decomposition\\var_clus.py\u001b[0m in \u001b[0;36m_decompose\u001b[1;34m(cluster, max_eigenvalue, max_tries)\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mmax_eigenvalue\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m            \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_split\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m            \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'explained_variance_'"
     ]
    }
   ],
   "source": [
    "n_features = 25\n",
    "n_rows = 1e4\n",
    "\n",
    "raw_df, _ = make_classification(n_samples=int(n_rows), \n",
    "                                n_features=n_features, \n",
    "                                n_informative=n_features,\n",
    "                                n_redundant=0)\n",
    "\n",
    "columns = ['feature_{}'.format(i) for i in range(n_features)]\n",
    "demo2_df = pd.DataFrame(raw_df, columns=columns)\n",
    "\n",
    "demo2 = VarClus(max_eigenvalue=1.1)\n",
    "demo2.decompose(demo2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The decomposition has a hierarchical structure, meaning the features of a child cluster are a subset of its parent cluster.\n",
    "The whole algorithm can be described as below**\n",
    "\n",
    ">> 1. Conducts PCA on current feature space. If the max eigenvalue is smaller than threshold,\n",
    "    stop decomposition\n",
    "2. Calculates the first N PCA components and assign features to these components based on\n",
    "    absolute correlation from high to low. These components are the initial centroids of\n",
    "    these child clusters.\n",
    "3. After initial assignment, the algorithm conducts an iterative assignment called Nearest\n",
    "    Component Sorting (NCS). Basically, the centroid vectors are re-computed as the first\n",
    "    components of the child clusters and the algorithm will re-assign each of the feature\n",
    "    based on the same correlation rule.\n",
    "4. After NCS, the algorithm tries to increase the total variance explained by the first\n",
    "    PCA component of each child cluster by re-assigning features across clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checkout the root_cluster\n",
    "root_cluster = demo2.cluster\n",
    "\n",
    "# Direct children of the root_cluster\n",
    "child_clusters = root_cluster.children\n",
    "\n",
    "# Direct parent of the root_cluster, if any\n",
    "parent_clusters = root_cluster.parents\n",
    "\n",
    "# root_cluster contains the original dataframe\n",
    "root_cluster.dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print out the structure of the decomposition\n",
    "demo2.print_cluster_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
